# jemdoc: menu{MENU}{index.html}
== Welcome to Fanghui Liu (刘方辉)'s Homepage!

~~~
{}{img_left}{images/me.jpg}{alt text}{200px}{200px}
= Fanghui Liu \n
=== Assistant Professor at [https://warwick.ac.uk/fac/sci/dcs/ Department of Computer Science, University of Warwick], UK \n
Email: *x*@*y* with *x*=fanghui.liu and *y*=warwick.ac.uk \n

Member of [https://warwick.ac.uk/fac/cross_fac/dimap/ Centre for Discrete Mathematics and its Applications (DIMAP)], Warwick

Visiting research scientist at [https://www.epfl.ch/labs/lions/people/guests/fanghui-liu-starts-1st-october/ EPFL], Switzerland \n
Email: *x*@*y* with *x*=fanghui.liu and *y*=epfl.ch \n

\[[https://scholar.google.com/citations?user=AKxBgssAAAAJ Google Scholar]\] 
\[[https://warwick.ac.uk/fac/sci/dcs/people/fanghui_liu/ homepage at Warwick]\]
\[[files/bio.text speaker bio]\]
\n
~~~

at Zermatt, Switzerland (Aug. 2022)


== About me

I'm currently an assistant professor at [https://warwick.ac.uk/fac/sci/dcs/ Department of Computer Science], a member of [https://warwick.ac.uk/fac/cross_fac/dimap/ Centre for Discrete Mathematics and its Applications (DIMAP)], an affiliated member of [https://warwick.ac.uk/fac/sci/dcs/research/focs/people/ the Division of Theory and Foundations (FoCS)], at the University of Warwick, UK.
Besides, I'm also a visiting research scientist at [https://www.epfl.ch/labs/lions/people/guests/fanghui-liu-starts-1st-october/ EPFL], Switzerland. I am an [https://ellis.eu/members/ ELLIS Member] and IEEE Senior Member.

We're organising [https://faiseminarswarwick.github.io/ *{{<font color="DarkMagenta">Warwick Foundation of AI seminar</font>}}*]!

We're organising one workshop [https://sites.google.com/view/neurips2024-ftw/home *{{<font color="red">Fine-Tuning in Modern Machine Learning: Principles and Scalability</font>}}*] at NeurIPS 2024!

Previously, I was a postdoc researcher at EPFL, Switzerland, hosted by [https://www.epfl.ch/labs/lions/people/volkan-cevher/ Prof. Volkan Cevher] from 2021 to 2023. 
Before that, I spent two years as a postdoc researcher at ESAT-STADIUS, KU Leuven, Belgium, hosted by [https://www.esat.kuleuven.be/sista/members/suykens.html Prof. Johan A.K. Suykens].

I obtained the Ph.D degree from Institute of Image Processing Pattern Recognition in Department of Automation, Shanghai Jiao Tong University (SJTU) in 
June 2019, supervised by [http://www.pami.sjtu.edu.cn/En/jieyang Prof. Jie Yang], 
and the B.E. degree in Automation, Harbin Institute of Technology (HIT) in 2014.


== Jobs

*{{<font color="red">I am looking for motivated Ph.D. students to work with me on learning theory in machine learning or trustworthy machine learning.</font>}}* See [https://www.lfhsgre.org/positions.html Open positions] for details. 

Due to a large number of requests, I, unfortunately, may not be able to reply to all the emails regarding PhD applications. However, I'll look at applicants that have sent me emails and contact you soon if your enquires are indeed inline with my research.

#Warwick has a strong research direction on TCS, mathematics, and statistics. If you would like to work with me on machine learning theory or theoretical-oriented application topics, feel free to reach out by sending me an email with your CV and transcript. Besides, I will appreciate if you include a short description about your motivation and some highlights of your past experience or achievements.

#== Education
#- Ph.D. in Department of Automation (Pattern Recognition and Intelligent Systems), Shanghai Jiao Tong University, Sep. 2014 - Jun. 2019
#- B.S. in Department of Automation, Harbin Institute of Technology, Sep. 2010 - Jun. 2014

== Research Interests 
#- Statistical machine learning, kernel methods, learning theory
#- Computer Vision

I'm generally interested in *mathematical foundations of machine learning*, 
 e.g., *statistical learning theory* and deep learning theory.
My research interest starts from kernel methods to large-scale computational methodologies in algorithm, and mainly focuses on theoretically understanding generalization properties of machine learning based algorithms, especially on *over-parameterized models* (motivated by neural networks). My research line can be understood from the perspective of *function space*, from RKHS to hyper-RKHS, and Barron spaces.
Besides, I also work on trustworthy machine learning, both theoretically and empirically.
My research (the past, ongoing and future) focuses on the following directions:

- machine learning theory: what is the largest function space that can be learned by neural networks,  both statistically and computationally efficiently (computational-statistical gaps)?
- fine-tuning:  expand the frontiers of empirical and theoretical knowledge on when and where to fine-tune, and how much we can fine-tune, precisely, efficiently, and robustly
- trustworthy machine learning



== News

- \[2024-09\] I will attend [https://www.crm.cat/mathematical-aspects-of-learning-theory/ Mathematical Aspects of Learning Theory Workshop - 20 years later].

- \[2024-08\] I will serve as an Area Chair of AAMAS 2025, ICLR 2025.

- \[2024-07\] We're organising one workshop [https://sites.google.com/view/neurips2024-ftw/home *{{<font color="red">Fine-Tuning in Modern Machine Learning: Principles and Scalability</font>}}*] at NeurIPS 2024!

- \[2024-05\] Two papers were accepted by ICML 2024: one is about high dimensional kernel methods under distribution shift; one is about adversarial attack on foundation models.

- \[2024-04\] One paper was accepted by JMLR on the separation between kernel methods and neural networks from the perspective of function space.

- \[2023-04\] We will give a tutorial entitled *Scaling and Reliability Foundations in Machine Learning* at 2024 IEEE International Symposium on Information Theory (ISIT) in Athens, Greece at July.

- \[2024-04\] Awarded the [https://www.daad.de/en/the-daad/postdocnet/details-and-application/ DAAD AInet Fellowship], which is awarded to outstanding early career researchers. Topic: Safety and Security in AI.

- \[2024-03\] The adversarially trained NAS benchmark (*NAS-RobBench-201*) in [https://openreview.net/pdf?id=cdUpf6t6LZ our ICLR24 paper] was released! See \[[https://tt2408.github.io/nasrobbench201hp/ project website]\] for details.

- \[2024-01\] Three papers were accepted by ICLR 2024: generalization of ResNets; robust NAS from benchmark to theory; local-linearity for catastrophic overfitting. *{{<font color="DarkMagenta">I will be at Vienna. Feel free to chat.</font>}}*

- \[2023-12\] Selected to give a talk at [https://aaai.org/aaai-conference/aaai-24-new-faculty-highlights/ AAAI 2024 New Faculty Highlights].

- \[2023-09\] Two papers were accepted by NeurIPS 2023: one is about global convergence of Transformers; the other one is how over-parameterization affects differential privacy. *{{<font color="DarkMagenta">I will be at New Orleans (again and again). Feel free to chat.</font>}}*

- \[2023-06\] Here is the slides of our [files/CVPR_tutorial.pdf CVPR 2023 tutorial].

- \[2023-06\] Here is the slides of our [files/ICASSP23_tutorial.pdf ICASSP 2023 tutorial].

- \[2023-04\] Two papers were accepted by ICML 2023: one is about function approximation in online RL; the other one is related to benign overfitting. *{{<font color="DarkMagenta">I will be at Hawaii! Feel free to chat.</font>}}*

- \[2023-04\] Selected as a [https://iclr.cc/Conferences/2023/Reviewers Notable Reviewer] for ICLR 2023.

- \[2023-02\] We will give a tutorial entitled *Deep learning theory for computer vision* at IEEE CVPR 2023 in Vancouver, Canada.

- \[2023-02\] I will attend [https://cemse.kaust.edu.sa/ai/aii-symp-2023 the Rising Stars in AI Symposium 2023] at KAUST in Saudi Arabia (Feb. 19-21).

- \[2022-12\] We will give a tutorial entitled *Neural networks: the good, the bad, the ugly* at IEEE ICASSP 2023 in the Greek island of Rhodes.

- \[2022-09\] Six papers were accepted by NeurIPS 2022.

- \[2021-10\] One paper on double descent of RFF with SGD was posted on 
[https://arxiv.org/abs/2110.06910 arXiv].

- \[2021-10\] One paper was accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence.

- \[2021-07\] One paper was accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence.

- \[2021-06\] One paper was accepted by Journal of Machine Learning Research.

- \[2021-02\] One paper was accepted by Machine Learning.

- \[2021-01\] Two papers were accepted by AISTATS 2021.

- \[2020-10\] One paper was accepted by Journal of Machine Learning Research.

== Visitors
~~~
{}{raw}
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=260&t=tt&d=CSj_lwkJPnjLuoGvBe6VD-xJaql8MX8sFoN-GPUfX38&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>
~~~
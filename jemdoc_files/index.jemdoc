# jemdoc: menu{MENU}{index.html}
== Welcome to Fanghui Liu (刘方辉)'s Homepage!

~~~
{}{img_left}{images/me.jpg}{alt text}{200px}{200px}
= Fanghui Liu \n
=== Postdoc researcher at [https://www.epfl.ch/labs/lions/ EPFL], Switzerland \n
Email: *x*@*y* with *x*=fanghui.liu and *y*=epfl.ch \n
\[[https://scholar.google.com/citations?user=AKxBgssAAAAJ Google Scholar]\] \n
~~~

at Zermatt, Switzerland (Aug. 2022)


== About me

I'm currently a postdoc researcher in the Laboratory for Information and Inference Systems 
(LIONS), École Polytechnique Fédérale de Lausanne (EPFL), Switzerland, 
working with [https://www.epfl.ch/labs/lions/people/volkan-cevher/ Prof. Volkan Cevher]. 
Before that, I spent two years as a postdoc researcher in Department of Electrical Engineering (ESAT-STADIUS), 
KU Leuven, Belgium, hosted by [https://www.esat.kuleuven.be/sista/members/suykens.html Prof. Johan A.K. Suykens]. I am an [https://ellis.eu/members/ ELLIS Member].

I obtained the Ph.D degree from Institute of Image Processing Pattern Recognition in Department of Automation, Shanghai Jiao Tong University (SJTU) in 
June 2019, supervised by [http://www.pami.sjtu.edu.cn/En/jieyang Prof. Jie Yang], 
and the B.E. degree in Automation, Harbin Institute of Technology (HIT) in 2014.

#== Education
#- Ph.D. in Department of Automation (Pattern Recognition and Intelligent Systems), Shanghai Jiao Tong University, Sep. 2014 - Jun. 2019
#- B.S. in Department of Automation, Harbin Institute of Technology, Sep. 2010 - Jun. 2014

== Research Interests 
#- Statistical machine learning, kernel methods, learning theory
#- Computer Vision

I'm generally interested in *mathematical foundations of machine learning*, 
 e.g., *statistical learning theory* and deep learning theory on the generalization properties of under-/over-parameterized models.
Currently I'm also interested in reinforcement learning theory, especially on function approximation, from classical statistical learning to sequential decision making.

My research interest starts from kernel methods to large-scale computational methodologies in algorithm, and mainly focuses on theoretically understanding generalization properties of machine learning based algorithms, especially on *over-parameterized models* (motivated by neural networks).
In fact, my research line can be understood from a *function space theory* perspective, from RKHS to hyper-RKHS, Barron spaces, Besov spaces.
This aims to understand the role of over-parameterization from kernel methods to neural networks.

Refer to my recent [files/Fanghui_talk.pdf *{{<font color="DarkMagenta">TALK</font>}}*] if you're interested in. 

- statistical learning theory, e.g., double descent, benign overfitting \[[http://proceedings.mlr.press/v130/liu21b.html AISTATS21], [https://arxiv.org/abs/2110.06910 NeurPS22], [https://arxiv.org/abs/2305.19377 ICML23]\], theory of kernel methods \[[https://jmlr.org/papers/v22/19-482.html JMLR21], [https://arxiv.org/abs/2006.01073 ML21]\].

- kernel methods and approximation \[[https://jmlr.org/papers/v21/19-900.html JMLR20], [https://arxiv.org/abs/2011.01668 TPAMI21], [http://proceedings.mlr.press/v130/liu21a.html AISTATS21] \]:
refer to a *{{<font color="DarkMagenta">survey</font>}}* \[[https://arxiv.org/abs/2004.11154 TPAMI21]\]!

- reinforcement leaning theory, e.g., function approximation \[[https://arxiv.org/abs/2209.07376 NeurIPS22], [https://arxiv.org/abs/2304.12886 ICML23]\]. 

Besides, I also spend time on some student projects in theoretical-oriented application topics,
e.g., robustness and verification \[[https://arxiv.org/abs/2209.07263 NeurIPS22], [https://arxiv.org/abs/2209.07235 NeurIPS22]\], neural architecture search \[[https://arxiv.org/abs/2209.07238 NeurIPS22]\], and out-of-distribution generalization \[[https://arxiv.org/abs/2209.07736 NeurIPS22]\], which aims to build *trustworthy machine learning* systems.
    





== News

- \[2023-06\] Here is the slides of our [files/CVPR_tutorial.pdf CVPR 2023 tutorial].

- \[2023-06\] Here is the slides of our [files/ICASSP23_tutorial.pdf ICASSP 2023 tutorial].

- \[2023-05\] One paper was accepted by TMLR regarding generalization guarantees of federated learning under covariate shifts.

- \[2023-04\] Two papers were accepted by ICML 2023: one is about function approximation in online RL; the other one is related to benign overfitting.

- \[2023-04\] I was selected as a [https://iclr.cc/Conferences/2023/Reviewers Notable Reviewer] for ICLR 2023.

- \[2023-02\] We will give a tutorial entitled *Deep learning theory for computer vision* at IEEE CVPR 2023 in Vancouver, Canada.

- \[2023-02\] I will attend [https://cemse.kaust.edu.sa/ai/aii-symp-2023 the Rising Stars in AI Symposium 2023] at KAUST in Saudi Arabia (Feb. 19-21).

- \[2022-12\] We will give a tutorial entitled *Neural networks: the good, the bad, the ugly* at IEEE ICASSP 2023 in the Greek island of Rhodes.

- \[2022-09\] Six papers were accepted by NeurIPS 2022.

- \[2021-10\] One paper on double descent of RFF with SGD was posted on 
[https://arxiv.org/abs/2110.06910 arXiv].

- \[2021-10\] One paper was accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence.

- \[2021-07\] One paper was accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence.

- \[2021-06\] One paper was accepted by Journal of Machine Learning Research.

- \[2021-02\] One paper was accepted by Machine Learning.

- \[2021-01\] Two papers were accepted by AISTATS 2021.

- \[2020-10\] One paper was accepted by Journal of Machine Learning Research.

== Visitors
~~~
{}{raw}
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=260&t=tt&d=CSj_lwkJPnjLuoGvBe6VD-xJaql8MX8sFoN-GPUfX38&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>
~~~
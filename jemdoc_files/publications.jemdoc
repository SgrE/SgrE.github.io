# jemdoc: menu{MENU}{publications.html}, nofooter
== Publications 

For the complete list, please refer to my [https://scholar.google.com/citations?user=AKxBgssAAAAJ Google Scholar Profile].

\* indicates equal contribution

== {{<font color="red">Benchmark</font>}}

The adversarially trained NAS benchmark (*NAS-RobBench-201*) in [https://openreview.net/pdf?id=cdUpf6t6LZ our ICLR24 paper] was released! See \[[https://tt2408.github.io/nasrobbench201hp/ project website]\] for details.


== Preprints

- /The Shape of Generalization through the Lens of Norm-based Capacity Control/. 
\[[https://arxiv.org/abs/2502.01585 arXiv]\]. \n
Yichen Wang, Yudong Chen, Lorenzo Rosasco, *Fanghui Liu*. \n
\[{{<font color="DarkMagenta">TLDR: We precisely characterise how the generalisation learning curve behaves under a suitable model capacity and rigorously prove (Fig. 8.12, CS229 Lecture Notes).</font>}}\]

- /Can overfitted deep neural networks in adversarial training generalize? -- An approximation viewpoint/. 
\[[https://arxiv.org/abs/2401.13624 arXiv]\]. \n
Zhongjie Shi, *Fanghui Liu*,  Yuan Cao, Johan A.K. Suykens. \n
\[{{<font color="DarkMagenta">TLDR: We rigorously prove when can we avoid robust overfitting from the approximation side.</font>}}\]

- /Benign overfitting in Fixed Dimension via Physics-Informed Learning with Smooth Inductive Bias/.
\[[https://arxiv.org/abs/2406.09194 arXiv]\]. \n
Honam Wang, Wendao Wu, *Fanghui Liu*, Yiping Lu


== Accepted Papers

- /LoRA-One: One-Step Full Gradient Could Suffice for Fine-Tuning Large Language Models, Provably and Efficiently/. 
\[[https://arxiv.org/abs/2502.01235 arXiv]\], \[[https://github.com/YuanheZ/LoRA-One code]\], [files/LoRA-One.gif GIF illustration]. \n
Yuanhe Zhang, *Fanghui Liu*,  Yudong Chen. \n
\[{{<font color="DarkMagenta">TLDR: We show how theory (from subspace alignment to gradient dynamics as well as preconditioners) contributes to fine-tuning algorithm design across several benchmarks.</font>}}\] \n
International Conference on Machine Learning (*ICML*), 2025. {{<font color="red">[Spotlight]</font>}} \n

- /How gradient descent balances features: A dynamical analysis for two-layer neural networks/. 
\[[https://openreview.net/forum?id=25j2ZEgwTj paper]\]. \n
Zhenyu Zhu, *Fanghui Liu*, Volkan Cevher \n
International Conference on Learning Representations (*ICLR*), 2025.

- /Learning with norm constrained, over-parameterised, two-layer neural networks/.
\[[https://arxiv.org/abs/2404.18769 arXiv]\] \n
*Fanghui Liu*, Leello Dadi, and Volkan Cevher. \n
Journal of Machine Learning Research (*JMLR*), 2024.\n
\[{{<font color="DarkMagenta">TLDR: We provide the "best" trade-off between sample complexity and the input dimension in high-dimensional machine learning. Our results can be also extended to the interpolation setting.</font>}}\]

- /Scalable Learned Model Soup on a Single GPU: An Efficient Subspace Training Strategy/.
\[[https://arxiv.org/abs/2407.03641 arXiv]\], \[[https://github.com/nblt/MEHL-Soup code]\] \n
Tao Li\*, Weisen Jiang\*, *Fanghui Liu*, Xiaolin Huang, James Kwok. \n
European Conference on Computer Vision (*ECCV*), 2024.

- /High-dimensional kernel methods under covariate shift: data-dependent implicit regularization/. 
\[[https://arxiv.org/abs/2406.03171 arXiv]\] \n
Yihang Chen, *Fanghui Liu*, Taiji Suzuki, Volkan Cevher. \n
in the 41st International Conference on Machine Learning (*ICML*), 2024. \n

- /Revisiting character-level adversarial attacks for language models/. 
\[[https://openreview.net/forum?id=AZWqXfM6z9 paper]\], \[[https://github.com/LIONS-EPFL/Charmer code]\]. \n
Elias Abad Rocamora, Yongtao Wu, *Fanghui Liu*, Grigorios Chrysos, Volkan Cevher. \n
in the 41st International Conference on Machine Learning (*ICML*), 2024. \n
Presented at ICLR 2024 Workshop on Secure and Trustworthy Large Language Models \n
\[{{<font color="DarkMagenta">TLDR: We introduce an efficient algorithm for character-level attack and typo-corrector doesn't work!</font>}}\]

- /Generalization of Deep ResNets in the mean-field regime/. 
\[[https://openreview.net/forum?id=tMzPZTvz2H link]\]. \n
Yihang Chen, *Fanghui Liu*, Yiping Lu, Grigorios Chrysos, Volkan Cevher. \n
in the 12th International Conference on Learning Representations (*ICLR*), 2024. {{<font color="red">[Spotlight]</font>}} \n

- /Robust NAS benchmark under adversarial training: assessment, theory, and beyond/. 
\[[https://openreview.net/forum?id=cdUpf6t6LZ paper]\], \[[https://tt2408.github.io/nasrobbench201hp/ project website]\]. \n
Yongtao Wu, *Fanghui Liu*, Carl-Johann Simon-Gabriel, Grigorios Chrysos, Volkan Cevher. \n
in the 12th International Conference on Learning Representations (*ICLR*), 2024. \n

- /Efficient local linearity regularization to overcome catastrophic overfitting/. 
\[[https://openreview.net/forum?id=SZzQz8ikwg paper]\], \[[https://github.com/LIONS-EPFL/ELLE code]\]. \n
Elias Abad Rocamora, *Fanghui Liu*, Grigorios Chrysos, Pablo M. Olmos, Volkan Cevher. \n
in the 12th International Conference on Learning Representations (*ICLR*), 2024. \n

- /On the convergence of encoder-only shallow Transformers/. 
\[[https://arxiv.org/abs/2311.01575 arxiv]\]. \n
Yongtao Wu, *Fanghui Liu*, Grigorios Chrysos, Volkan Cevher. \n
in the 37th Conference on Neural Information Processing Systems (*NeurIPS*), 2023. \n

- /Initialization matters: Privacy-utility analysis of overparameterized neural networks/. 
\[[https://arxiv.org/abs/2310.20579 arXiv]\]. \n
Jiayuan Ye, Zhenyu Zhu, *Fanghui Liu*, Reza Shokri, Volkan Cevher. \n
in the 37th Conference on Neural Information Processing Systems (*NeurIPS*), 2023. \n

- /What can online reinforcement learning with function approximation benefit from general coverage conditions?/. 
\[[https://arxiv.org/abs/2304.12886 arXiv]\]. \n
*Fanghui Liu*, Luca Viano, Volkan Cevher. \n
in the 40th International Conference on Machine Learning (*ICML*), 2023. \n

- /Benign Overfitting in Deep Neural Networks under Lazy Training/. 
\[[https://arxiv.org/abs/2305.19377 arXiv]\]. \n
Zhenyu Zhu, *Fanghui Liu*,  Grigorios Chrysos, Francesco Locatello, Volkan Cevher. \n
in the 40th International Conference on Machine Learning (*ICML*), 2023. \n

- /On the double descent of random features models trained by SGD/. 
\[[https://arxiv.org/abs/2110.06910 arXiv]\],
\[[files/code_DD_RFF.zip code]\], \[[files/Fanghui_DD.pdf slides]\]. \n
*Fanghui Liu*, Johan A.K. Suykens, Volkan Cevher. \n
in the 36th Conference on Neural Information Processing Systems (*NeurIPS*), 2022. \n
Presented at [https://topml.rice.edu/ Workshop on the Theory of Overparameterized Machine Learning (TOPML) 2022].\n
\[{{<font color="DarkMagenta">TLDR: We study the double descent, interplay with the data, parameter, and compute budget (scaling law), allowing for obtaining dimension-free results.</font>}}\]

- /Understanding deep neural function approximation in reinforcement learning via $\epsilon$-greedy exploration/. 
\[[https://arxiv.org/abs/2209.07376 arXiv]\]. \n
*Fanghui Liu*, Luca Viano, Volkan Cevher. \n
in the 36th Conference on Neural Information Processing Systems (*NeurIPS*), 2022. \n
\[{{<font color="DarkMagenta">TLDR: This is the attempt for nonlinear function approximation in online RL via neural networks beyond lazy training.</font>}}\]

- /Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization)/. 
\[[https://arxiv.org/abs/2209.07263 arXiv]\], \[[files/NeurIPS22-robustness.pdf slides]\]. \n
Zhenyu Zhu, *Fanghui Liu*,  Grigorios Chrysos, Volkan Cevher. \n
in the 36th Conference on Neural Information Processing Systems (*NeurIPS*), 2022. \n
\[{{<font color="DarkMagenta">We aim to close the gap on the question: will over-parameterisation help or hurt robustness?</font>}}\]

- /Generalization properties of NAS under activation and skip connection search/.
\[[https://arxiv.org/abs/2209.07238 arXiv]\]. \n
Zhenyu Zhu, *Fanghui Liu*, Grigorios Chrysos, Volkan Cevher. \n
in the 36th Conference on Neural Information Processing Systems (*NeurIPS*), 2022. \n

- /Extrapolation and spectral bias of neural nets with Hadamard product: a polynomial net study/.
\[[https://arxiv.org/abs/2209.07736 arXiv]\]. \n
Yongtao Wu, Zhenyu Zhu, *Fanghui Liu*, Grigorios Chrysos, Volkan Cevher. \n
in the 36th Conference on Neural Information Processing Systems (*NeurIPS*), 2022. \n

- /Sound and complete verification of polynomial networks/. \[[https://arxiv.org/abs/2209.07235 arXiv]\]. \n
Elias Abad Rocamora, Mehmet Fatih Sahin, *Fanghui Liu*, Grigorios Chrysos, Volkan Cevher. \n
in the 36th Conference on Neural Information Processing Systems (*NeurIPS*), 2022. \n

- /Random features for kernel approximation: A Survey on algorithms, theory, and beyond/.
\[[https://arxiv.org/abs/2004.11154 arXiv]\], \[[files/RFFsurvey_demo.zip code]\]. \n
*Fanghui Liu*, Xiaolin Huang, Yudong Chen, and Johan A.K. Suykens. \n
IEEE Transactions on Pattern Analysis and Machine Intelligence (*TPAMI*), 2021. \n
\[{{<font color="DarkMagenta">TLDR: This is a comprehensive survey summarising random features from algorithm to theory. The over-parameterisation part does not involve too much.</font>}}\]

- /Generalization properties of hyper-RKHS and its applications/.
\[[https://arxiv.org/abs/1809.09910 arxiv]\], \[[https://jmlr.org/papers/v22/19-482.html link]\], \[[files/demo-hyperRKHS.zip code]\]. \n
*Fanghui Liu*\*, Lei Shi\*, Xiaolin Huang, Jie Yang, and Johan A.K. Suykens. \n
Journal of Machine Learning Research (*JMLR*), 2021. \n
\[{{<font color="DarkMagenta">TLDR: This work provides analysis on learning beyond RKHS with non-trivial concentration inequality for dependence.</font>}}\]

- /Towards a unified quadrature framework for large scale kernel methods/.
\[[https://arxiv.org/abs/2011.01668 arXiv]\], \[[files/Quadrature_demo.zip code]\]. \n
*Fanghui Liu*, Xiaolin Huang, Yudong Chen, and Johan A.K. Suykens.\n
IEEE Transactions on Pattern Analysis and Machine Intelligence (*TPAMI*), 2021.

- /Kernel regression in high dimensions: Refined analysis beyond double descent/.
\[[http://proceedings.mlr.press/v130/liu21b.html link]\], \[[files/demo_KRRhigh.m code]\], \[[files/Fanghui_KRR.pdf slides]\]. \n
*Fanghui Liu*, Zhenyu Liao, and Johan A.K. Suykens.\n 
in the 24th International Conference on Artificial Intelligence and Statistics (*AISTATS*), 2021.\n
\[{{<font color="DarkMagenta">TLDR: This work extends the double descent theory.</font>}}\]

- /Fast learning in reproducing kernel Krein spaces via signed measures/.
\[[http://proceedings.mlr.press/v130/liu21a.html link]\], \[[files/RFF_RKKS_poster.pdf poster]\],
 \[[files/code-RFF_RKKS.zip code]\]. \n
*Fanghui Liu*, Xiaolin Huang, Yingyi Chen, and Johan A.K. Suykens. \n
in the 24th International Conference on Artificial Intelligence and Statistics (*AISTATS*), 2021.

- /Analysis of least squares regularized regression in reproducing kernel Krein spaces/.
\[[https://arxiv.org/abs/2006.01073 arXiv]\]. \n
*Fanghui Liu*\*, Lei Shi\*, Xiaolin Huang, Jie Yang, and Johan A.K. Suykens. \n
*Machine Learning*, 2021. \n
\[{{<font color="DarkMagenta">TLDR: This work develop new proof framework to handle non-positive kernel in learning theory.</font>}}\]

- /Learning data-adaptive nonparametric kernels/.
\[[https://jmlr.org/papers/v21/19-900.html link]\] \[[files/NesterovAcc.m code]\]. \n
*Fanghui Liu*, Xiaolin Huang, Chen Gong, Jie Yang, and Li Li. \n
Journal of Machine Learning Research (*JMLR*), 2020.

- /Random Fourier features via fast surrogate leverage weighted sampling/.
\[[https://arxiv.org/abs/1911.09158 arXiv]\], \[[files/AAAI20_code.zip code]\].\n 
*Fanghui Liu*, Xiaolin Huang, Yudong Chen, Jie Yang, and Johan A.K. Suykens. \n
in the Thirty-Fourth AAAI Conference on Artificial Intelligence (*AAAI*), 2020. \n
\[{{<font color="DarkMagenta">TLDR: This work presents an efficient algorithm of leverage score for large-scale kernel approximation, motived by kernel alignment.</font>}}\]

- /A double-variational Bayesian framework in random Fourier features 
 for indefinite kernels/.
\[[https://lirias.kuleuven.be/retrieve/554716 link]\], \[[files/RFF-DIGMM_code.zip code]\].\n
*Fanghui Liu*, Xiaolin Huang, Lei Shi, Jie Yang, and Johan A.K. Suykens.\n 
IEEE Transactions on Neural Networks and Learning Systems (*TNNLS*), 2019.

- /Indefinite kernel logistic regression with Concave-inexact-convex procedure/.
\[[https://arxiv.org/abs/1707.01826 arXiv]\], \[[files/IKLR_code.zip code]\]. \n
*Fanghui Liu*, Xiaolin Huang, Chen Gong, Jie Yang, and Johan A.K. Suykens. \n
IEEE Transactions on Neural Networks and Learning Systems (*TNNLS*), 2018.

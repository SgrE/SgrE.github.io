<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="icon" href="images/icon.png">
<link rel="stylesheet" href="main.css" type="text/css" />
<link rel="stylesheet" href="font-awesome/css/font-awesome.min.css">
<!--- <title>Publications </title> --->
<title>Fanghui Liu</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<div id="main-container">
<div id="header-container">
<div id="header">
<div id="header-icon-text-container">
<div id="header-icon-container" >
<a href="index.html"><img src="images/profile2.png" alt="" style="width: 100%; height: 100%; position: center; padding:0px; margin: 0px;"></a>
</div>
<div id="header-text-container">
<a href="index.html">Fanghui Liu</a>
</div>
</div>
<div id="main">
<button class="openbtn" onclick="openNav()">☰</button>
</div>
</div>
</div>
<div id="layout">
<div id="layout-menu-container">
<div id="layout-menu">
<div class="menu-item"><a href="javascript:void(0)" class="closebtn" onclick="closeNav()">×</a></div>
<div class="menu-item"><a href="index.html">About</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="publications.html" class="current">Publications</a></div>
<div class="menu-item"><a href="talks.html">Talks</a></div>
<div class="menu-item"><a href="awards.html">Awards</a></div>
<div class="menu-category">Teaching</div>
<div class="menu-item"><a href="teaching.html">Courses</a></div>
<div class="menu-item"><a href="tutorials.html">Tutorials</a></div>
<div class="menu-category">Supervision</div>
<div class="menu-item"><a href="students.html">Students</a></div>
<div class="menu-item"><a href="positions.html">Open&nbsp;Positions</a></div>
<div class="menu-category">Service</div>
<div class="menu-item"><a href="services.html">Academic&nbsp;Service</a></div>
<div class="menu-item"><a href="workshop.html">NeurIPS&rsquo;24&nbsp;FITML&nbsp;Workshop</a></div>
</div> <!-- <div id="layout-menu"> -->
</div> <!-- <div id="layout-menu-container"> -->
<div id="layout-content-container">
<div id="layout-content">
<div id="toptitle">
<h1>Publications </h1>
</div>
<p>For the complete list, please refer to my <a href="https://scholar.google.com/citations?user=AKxBgssAAAAJ" target=&ldquo;blank&rdquo;>Google Scholar Profile</a>.</p>
<p>* indicates equal contribution</p>
<h2><font color="red">Benchmark</font></h2>
<p>The adversarially trained NAS benchmark (<b>NAS-RobBench-201</b>) in <a href="https://openreview.net/pdf?id=cdUpf6t6LZ" target=&ldquo;blank&rdquo;>our ICLR24 paper</a> was released! See [<a href="https://tt2408.github.io/nasrobbench201hp/" target=&ldquo;blank&rdquo;>project website</a>] for details.</p>
<h2>Preprints</h2>
<ul>
<li><p><i>Can overfitted deep neural networks in adversarial training generalize? &ndash; An approximation viewpoint</i>. 
[<a href="https://arxiv.org/abs/2401.13624" target=&ldquo;blank&rdquo;>arXiv</a>]. <br />
Zhongjie Shi, <b>Fanghui Liu</b>,  Yuan Cao, Johan A.K. Suykens. <br />
[<font color="DarkMagenta">TLDR: We rigorously prove when can we avoid robust overfitting from the approximation side.</font>]</p>
</li>
</ul>
<ul>
<li><p><i>Benign overfitting in Fixed Dimension via Physics-Informed Learning with Smooth Inductive Bias</i>.
[<a href="https://arxiv.org/abs/2406.09194" target=&ldquo;blank&rdquo;>arXiv</a>]. <br />
Honam Wang, Wendao Wu, <b>Fanghui Liu</b>, Yiping Lu</p>
</li>
</ul>
<h2>Accepted Papers</h2>
<ul>
<li><p><i>The Shape of Generalization through the Lens of Norm-based Capacity Control</i>. 
[<a href="https://arxiv.org/abs/2502.01585" target=&ldquo;blank&rdquo;>arXiv</a>]. <br />
Yichen Wang, Yudong Chen, Lorenzo Rosasco, <b>Fanghui Liu</b>. <br />
in the 39th Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2025. <br />
[<font color="DarkMagenta">TLDR: We precisely characterize how the generalization learning curve behaves under a suitable model capacity and rigorously prove (Fig. 8.12, CS229 Lecture Notes).</font>]</p>
</li>
</ul>
<ul>
<li><p><i>LoRA-One: One-Step Full Gradient Could Suffice for Fine-Tuning Large Language Models, Provably and Efficiently</i>. 
[<a href="https://arxiv.org/abs/2502.01235" target=&ldquo;blank&rdquo;>arXiv</a>], [<a href="https://github.com/YuanheZ/LoRA-One" target=&ldquo;blank&rdquo;>code</a>], [<a href="files/LoRA-One.gif" target=&ldquo;blank&rdquo;>GIF illustration</a>], [<a href="https://yuanhez.github.io/loraoneofficial/" target=&ldquo;blank&rdquo;>project website</a>]. <br />
Yuanhe Zhang, <b>Fanghui Liu</b>,  Yudong Chen. <br />
International Conference on Machine Learning (<b>ICML</b>), 2025. <font color="red">[Oral]</font> <br />
[<font color="DarkMagenta">TLDR: We show how theory (from subspace alignment to gradient dynamics as well as preconditioners) contributes to fine-tuning algorithm design in practice.</font>]</p>
</li>
</ul>
<ul>
<li><p><i>How gradient descent balances features: A dynamical analysis for two-layer neural networks</i>. 
[<a href="https://openreview.net/forum?id=25j2ZEgwTj" target=&ldquo;blank&rdquo;>paper</a>]. <br />
Zhenyu Zhu, <b>Fanghui Liu</b>, Volkan Cevher <br />
International Conference on Learning Representations (<b>ICLR</b>), 2025.</p>
</li>
</ul>
<ul>
<li><p><i>Learning with norm constrained, over-parameterised, two-layer neural networks</i>.
[<a href="https://arxiv.org/abs/2404.18769" target=&ldquo;blank&rdquo;>arXiv</a>] <br />
<b>Fanghui Liu</b>, Leello Dadi, and Volkan Cevher. <br />
Journal of Machine Learning Research (<b>JMLR</b>), 2024.<br />
[<font color="DarkMagenta">TLDR: We provide the "best" trade-off between sample complexity and the input dimension in high-dimensional machine learning. Our results can be also extended to the interpolation setting.</font>]</p>
</li>
</ul>
<ul>
<li><p><i>Scalable Learned Model Soup on a Single GPU: An Efficient Subspace Training Strategy</i>.
[<a href="https://arxiv.org/abs/2407.03641" target=&ldquo;blank&rdquo;>arXiv</a>], [<a href="https://github.com/nblt/MEHL-Soup" target=&ldquo;blank&rdquo;>code</a>] <br />
Tao Li*, Weisen Jiang*, <b>Fanghui Liu</b>, Xiaolin Huang, James Kwok. <br />
European Conference on Computer Vision (<b>ECCV</b>), 2024.</p>
</li>
</ul>
<ul>
<li><p><i>High-dimensional kernel methods under covariate shift: data-dependent implicit regularization</i>. 
[<a href="https://arxiv.org/abs/2406.03171" target=&ldquo;blank&rdquo;>arXiv</a>] <br />
Yihang Chen, <b>Fanghui Liu</b>, Taiji Suzuki, Volkan Cevher. <br />
in the 41st International Conference on Machine Learning (<b>ICML</b>), 2024. <br /></p>
</li>
</ul>
<ul>
<li><p><i>Revisiting character-level adversarial attacks for language models</i>. 
[<a href="https://openreview.net/forum?id=AZWqXfM6z9" target=&ldquo;blank&rdquo;>paper</a>], [<a href="https://github.com/LIONS-EPFL/Charmer" target=&ldquo;blank&rdquo;>code</a>]. <br />
Elias Abad Rocamora, Yongtao Wu, <b>Fanghui Liu</b>, Grigorios Chrysos, Volkan Cevher. <br />
in the 41st International Conference on Machine Learning (<b>ICML</b>), 2024. <br />
Presented at ICLR 2024 Workshop on Secure and Trustworthy Large Language Models <br />
[<font color="DarkMagenta">TLDR: We introduce an efficient algorithm for character-level attack and typo-corrector doesn't work!</font>]</p>
</li>
</ul>
<ul>
<li><p><i>Generalization of Deep ResNets in the mean-field regime</i>. 
[<a href="https://openreview.net/forum?id=tMzPZTvz2H" target=&ldquo;blank&rdquo;>link</a>]. <br />
Yihang Chen, <b>Fanghui Liu</b>, Yiping Lu, Grigorios Chrysos, Volkan Cevher. <br />
in the 12th International Conference on Learning Representations (<b>ICLR</b>), 2024. <font color="red">[Spotlight]</font> <br /></p>
</li>
</ul>
<ul>
<li><p><i>Robust NAS benchmark under adversarial training: assessment, theory, and beyond</i>. 
[<a href="https://openreview.net/forum?id=cdUpf6t6LZ" target=&ldquo;blank&rdquo;>paper</a>], [<a href="https://tt2408.github.io/nasrobbench201hp/" target=&ldquo;blank&rdquo;>project website</a>]. <br />
Yongtao Wu, <b>Fanghui Liu</b>, Carl-Johann Simon-Gabriel, Grigorios Chrysos, Volkan Cevher. <br />
in the 12th International Conference on Learning Representations (<b>ICLR</b>), 2024. <br /></p>
</li>
</ul>
<ul>
<li><p><i>Efficient local linearity regularization to overcome catastrophic overfitting</i>. 
[<a href="https://openreview.net/forum?id=SZzQz8ikwg" target=&ldquo;blank&rdquo;>paper</a>], [<a href="https://github.com/LIONS-EPFL/ELLE" target=&ldquo;blank&rdquo;>code</a>]. <br />
Elias Abad Rocamora, <b>Fanghui Liu</b>, Grigorios Chrysos, Pablo M. Olmos, Volkan Cevher. <br />
in the 12th International Conference on Learning Representations (<b>ICLR</b>), 2024. <br /></p>
</li>
</ul>
<ul>
<li><p><i>On the convergence of encoder-only shallow Transformers</i>. 
[<a href="https://arxiv.org/abs/2311.01575" target=&ldquo;blank&rdquo;>arxiv</a>]. <br />
Yongtao Wu, <b>Fanghui Liu</b>, Grigorios Chrysos, Volkan Cevher. <br />
in the 37th Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2023. <br /></p>
</li>
</ul>
<ul>
<li><p><i>Initialization matters: Privacy-utility analysis of overparameterized neural networks</i>. 
[<a href="https://arxiv.org/abs/2310.20579" target=&ldquo;blank&rdquo;>arXiv</a>]. <br />
Jiayuan Ye, Zhenyu Zhu, <b>Fanghui Liu</b>, Reza Shokri, Volkan Cevher. <br />
in the 37th Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2023. <br /></p>
</li>
</ul>
<ul>
<li><p><i>What can online reinforcement learning with function approximation benefit from general coverage conditions?</i>. 
[<a href="https://arxiv.org/abs/2304.12886" target=&ldquo;blank&rdquo;>arXiv</a>]. <br />
<b>Fanghui Liu</b>, Luca Viano, Volkan Cevher. <br />
in the 40th International Conference on Machine Learning (<b>ICML</b>), 2023. <br /></p>
</li>
</ul>
<ul>
<li><p><i>Benign Overfitting in Deep Neural Networks under Lazy Training</i>. 
[<a href="https://arxiv.org/abs/2305.19377" target=&ldquo;blank&rdquo;>arXiv</a>]. <br />
Zhenyu Zhu, <b>Fanghui Liu</b>,  Grigorios Chrysos, Francesco Locatello, Volkan Cevher. <br />
in the 40th International Conference on Machine Learning (<b>ICML</b>), 2023. <br /></p>
</li>
</ul>
<ul>
<li><p><i>On the double descent of random features models trained by SGD</i>. 
[<a href="https://arxiv.org/abs/2110.06910" target=&ldquo;blank&rdquo;>arXiv</a>],
[<a href="files/code_DD_RFF.zip" target=&ldquo;blank&rdquo;>code</a>], [<a href="files/Fanghui_DD.pdf" target=&ldquo;blank&rdquo;>slides</a>]. <br />
<b>Fanghui Liu</b>, Johan A.K. Suykens, Volkan Cevher. <br />
in the 36th Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2022. <br />
Presented at <a href="https://topml.rice.edu/" target=&ldquo;blank&rdquo;>Workshop on the Theory of Overparameterized Machine Learning (TOPML) 2022</a>.<br />
[<font color="DarkMagenta">TLDR: We study the double descent, interplay with the data, parameter, and compute budget (scaling law), allowing for obtaining dimension-free results.</font>]</p>
</li>
</ul>
<ul>
<li><p><i>Understanding deep neural function approximation in reinforcement learning via \(\epsilon\)-greedy exploration</i>. 
[<a href="https://arxiv.org/abs/2209.07376" target=&ldquo;blank&rdquo;>arXiv</a>]. <br />
<b>Fanghui Liu</b>, Luca Viano, Volkan Cevher. <br />
in the 36th Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2022. <br />
[<font color="DarkMagenta">TLDR: This is the attempt for nonlinear function approximation in online RL via neural networks beyond lazy training.</font>]</p>
</li>
</ul>
<ul>
<li><p><i>Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization)</i>. 
[<a href="https://arxiv.org/abs/2209.07263" target=&ldquo;blank&rdquo;>arXiv</a>], [<a href="files/NeurIPS22-robustness.pdf" target=&ldquo;blank&rdquo;>slides</a>]. <br />
Zhenyu Zhu, <b>Fanghui Liu</b>,  Grigorios Chrysos, Volkan Cevher. <br />
in the 36th Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2022. <br />
[<font color="DarkMagenta">We aim to close the gap on the question: will over-parameterisation help or hurt robustness?</font>]</p>
</li>
</ul>
<ul>
<li><p><i>Generalization properties of NAS under activation and skip connection search</i>.
[<a href="https://arxiv.org/abs/2209.07238" target=&ldquo;blank&rdquo;>arXiv</a>]. <br />
Zhenyu Zhu, <b>Fanghui Liu</b>, Grigorios Chrysos, Volkan Cevher. <br />
in the 36th Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2022. <br /></p>
</li>
</ul>
<ul>
<li><p><i>Extrapolation and spectral bias of neural nets with Hadamard product: a polynomial net study</i>.
[<a href="https://arxiv.org/abs/2209.07736" target=&ldquo;blank&rdquo;>arXiv</a>]. <br />
Yongtao Wu, Zhenyu Zhu, <b>Fanghui Liu</b>, Grigorios Chrysos, Volkan Cevher. <br />
in the 36th Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2022. <br /></p>
</li>
</ul>
<ul>
<li><p><i>Sound and complete verification of polynomial networks</i>. [<a href="https://arxiv.org/abs/2209.07235" target=&ldquo;blank&rdquo;>arXiv</a>]. <br />
Elias Abad Rocamora, Mehmet Fatih Sahin, <b>Fanghui Liu</b>, Grigorios Chrysos, Volkan Cevher. <br />
in the 36th Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2022. <br /></p>
</li>
</ul>
<ul>
<li><p><i>Random features for kernel approximation: A Survey on algorithms, theory, and beyond</i>.
[<a href="https://arxiv.org/abs/2004.11154" target=&ldquo;blank&rdquo;>arXiv</a>], [<a href="files/RFFsurvey_demo.zip" target=&ldquo;blank&rdquo;>code</a>]. <br />
<b>Fanghui Liu</b>, Xiaolin Huang, Yudong Chen, and Johan A.K. Suykens. <br />
IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2021. <br />
[<font color="DarkMagenta">TLDR: This is a comprehensive survey summarising random features from algorithm to theory. The over-parameterisation part does not involve too much.</font>]</p>
</li>
</ul>
<ul>
<li><p><i>Generalization properties of hyper-RKHS and its applications</i>.
[<a href="https://arxiv.org/abs/1809.09910" target=&ldquo;blank&rdquo;>arxiv</a>], [<a href="https://jmlr.org/papers/v22/19-482.html" target=&ldquo;blank&rdquo;>link</a>], [<a href="files/demo-hyperRKHS.zip" target=&ldquo;blank&rdquo;>code</a>]. <br />
<b>Fanghui Liu</b>*, Lei Shi*, Xiaolin Huang, Jie Yang, and Johan A.K. Suykens. <br />
Journal of Machine Learning Research (<b>JMLR</b>), 2021. <br />
[<font color="DarkMagenta">TLDR: This work provides analysis on learning beyond RKHS with non-trivial concentration inequality for dependence.</font>]</p>
</li>
</ul>
<ul>
<li><p><i>Towards a unified quadrature framework for large scale kernel methods</i>.
[<a href="https://arxiv.org/abs/2011.01668" target=&ldquo;blank&rdquo;>arXiv</a>], [<a href="files/Quadrature_demo.zip" target=&ldquo;blank&rdquo;>code</a>]. <br />
<b>Fanghui Liu</b>, Xiaolin Huang, Yudong Chen, and Johan A.K. Suykens.<br />
IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2021.</p>
</li>
</ul>
<ul>
<li><p><i>Kernel regression in high dimensions: Refined analysis beyond double descent</i>.
[<a href="http://proceedings.mlr.press/v130/liu21b.html" target=&ldquo;blank&rdquo;>link</a>], [<a href="files/demo_KRRhigh.m" target=&ldquo;blank&rdquo;>code</a>], [<a href="files/Fanghui_KRR.pdf" target=&ldquo;blank&rdquo;>slides</a>]. <br />
<b>Fanghui Liu</b>, Zhenyu Liao, and Johan A.K. Suykens.<br /> 
in the 24th International Conference on Artificial Intelligence and Statistics (<b>AISTATS</b>), 2021.<br />
[<font color="DarkMagenta">TLDR: This work extends the double descent theory.</font>]</p>
</li>
</ul>
<ul>
<li><p><i>Fast learning in reproducing kernel Krein spaces via signed measures</i>.
[<a href="http://proceedings.mlr.press/v130/liu21a.html" target=&ldquo;blank&rdquo;>link</a>], [<a href="files/RFF_RKKS_poster.pdf" target=&ldquo;blank&rdquo;>poster</a>],
[<a href="files/code-RFF_RKKS.zip" target=&ldquo;blank&rdquo;>code</a>]. <br />
<b>Fanghui Liu</b>, Xiaolin Huang, Yingyi Chen, and Johan A.K. Suykens. <br />
in the 24th International Conference on Artificial Intelligence and Statistics (<b>AISTATS</b>), 2021.</p>
</li>
</ul>
<ul>
<li><p><i>Analysis of least squares regularized regression in reproducing kernel Krein spaces</i>.
[<a href="https://arxiv.org/abs/2006.01073" target=&ldquo;blank&rdquo;>arXiv</a>]. <br />
<b>Fanghui Liu</b>*, Lei Shi*, Xiaolin Huang, Jie Yang, and Johan A.K. Suykens. <br />
<b>Machine Learning</b>, 2021. <br />
[<font color="DarkMagenta">TLDR: This work develop new proof framework to handle non-positive kernel in learning theory.</font>]</p>
</li>
</ul>
<ul>
<li><p><i>Learning data-adaptive nonparametric kernels</i>.
[<a href="https://jmlr.org/papers/v21/19-900.html" target=&ldquo;blank&rdquo;>link</a>] [<a href="files/NesterovAcc.m" target=&ldquo;blank&rdquo;>code</a>]. <br />
<b>Fanghui Liu</b>, Xiaolin Huang, Chen Gong, Jie Yang, and Li Li. <br />
Journal of Machine Learning Research (<b>JMLR</b>), 2020.</p>
</li>
</ul>
<ul>
<li><p><i>Random Fourier features via fast surrogate leverage weighted sampling</i>.
[<a href="https://arxiv.org/abs/1911.09158" target=&ldquo;blank&rdquo;>arXiv</a>], [<a href="files/AAAI20_code.zip" target=&ldquo;blank&rdquo;>code</a>].<br /> 
<b>Fanghui Liu</b>, Xiaolin Huang, Yudong Chen, Jie Yang, and Johan A.K. Suykens. <br />
in the Thirty-Fourth AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2020. <br />
[<font color="DarkMagenta">TLDR: This work presents an efficient algorithm of leverage score for large-scale kernel approximation, motived by kernel alignment.</font>]</p>
</li>
</ul>
<ul>
<li><p><i>A double-variational Bayesian framework in random Fourier features 
for indefinite kernels</i>.
[<a href="https://lirias.kuleuven.be/retrieve/554716" target=&ldquo;blank&rdquo;>link</a>], [<a href="files/RFF-DIGMM_code.zip" target=&ldquo;blank&rdquo;>code</a>].<br />
<b>Fanghui Liu</b>, Xiaolin Huang, Lei Shi, Jie Yang, and Johan A.K. Suykens.<br /> 
IEEE Transactions on Neural Networks and Learning Systems (<b>TNNLS</b>), 2019.</p>
</li>
</ul>
<ul>
<li><p><i>Indefinite kernel logistic regression with Concave-inexact-convex procedure</i>.
[<a href="https://arxiv.org/abs/1707.01826" target=&ldquo;blank&rdquo;>arXiv</a>], [<a href="files/IKLR_code.zip" target=&ldquo;blank&rdquo;>code</a>]. <br />
<b>Fanghui Liu</b>, Xiaolin Huang, Chen Gong, Jie Yang, and Johan A.K. Suykens. <br />
IEEE Transactions on Neural Networks and Learning Systems (<b>TNNLS</b>), 2018.</p>
</li>
</ul>
</div> <!-- <div id="layout-content-container"> -->
</div> <!--- <div id="layout"> --->
</div> <!--- <div id="main-container"> --->
<script>
function openNav() {
    if (window.innerWidth <= 1200) {
        document.getElementById("layout-menu").style.width = "280px";
        document.getElementById("layout-content-container").style.marginLeft = "280.8px";
        document.getElementById("layout-content-container").style.position = "fixed";
    }
}
function closeNav() {
    if (window.innerWidth <= 1200) {
        document.getElementById("layout-menu").style.width = "0";
        document.getElementById("layout-content-container").style.position = "static";
        document.getElementById("layout-content-container").style.marginLeft = "0px";
        setInterval(
            function(){ location.reload() },
            500
        );
    }
}
</script>
</body>
</html>
